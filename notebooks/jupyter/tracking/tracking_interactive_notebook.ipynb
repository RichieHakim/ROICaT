{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f643d500",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b61fc951",
   "metadata": {},
   "source": [
    "Welcome to the interactive tracking notebook!\\\n",
    "This notebook goes through each step and allows you to tune parameters and view how it changes the results.\n",
    "\n",
    "The notebook proceeds as follows:\n",
    "1. **Import** libraries\n",
    "2. Define **paths** to data\n",
    "3. Run data through the **pipeline**. Each step of the pipeline is run by a single unique python class.\n",
    "4. **Visualize** results\n",
    "5. **Save** results\n",
    "\n",
    "As you go through the notebook, take note of the small number of parameters that are mentioned as **'important parameters'** (consider searching for these in the notebook). We consider these to be the only parameters that can have a large effect on the run output. Other parameters matter and should be considered as well, but are less critical."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50622a4e",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9824970",
   "metadata": {},
   "source": [
    "Widen the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997af9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# widen jupyter notebook window\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:95% !important; }</style>\"))\n",
    "display(HTML(\"<style>:root { --jp-notebook-max-width: 100% !important; }</style>\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0cbc7a4",
   "metadata": {},
   "source": [
    "Import basic libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eb1fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import copy\n",
    "import multiprocessing as mp\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f24c78a",
   "metadata": {},
   "source": [
    "Import `roicat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7f4f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import roicat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8adf6d9",
   "metadata": {},
   "source": [
    "# Find paths to data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c8437c9",
   "metadata": {},
   "source": [
    "In this example we are using suite2p output files, but other data types can be used (CaImAn, etc.) \\\n",
    "See the notebook on ingesting diverse data: https://github.com/RichieHakim/ROICaT/blob/main/notebooks/jupyter/other/demo_custom_data_importing.ipynb\n",
    "\n",
    "Make a list containing the paths to all the input files.\n",
    "\n",
    "In this example we are using suite2p, so the following are defined:\n",
    "1. `paths_allStat`: a list to all the stat.npy files\n",
    "2. `paths_allOps`: a list with ops.npy files that correspond 1-to-1 with the stat.npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de208487-2702-4bd7-96be-4e62e8948de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_allOuterFolders = r'/media/rich/bigSSD/analysis_data/face_rhythm/mouse_0403L/stat_and_ops/'\n",
    "\n",
    "pathSuffixToStat = 'stat.npy'\n",
    "pathSuffixToOps = 'ops.npy'\n",
    "\n",
    "paths_allStat = roicat.helpers.find_paths(\n",
    "    dir_outer=dir_allOuterFolders,\n",
    "    reMatch=pathSuffixToStat,\n",
    "    depth=6,\n",
    ")[:]\n",
    "paths_allOps  = np.array([Path(path).resolve().parent / pathSuffixToOps for path in paths_allStat])[:]\n",
    "\n",
    "print(f'paths to all stat files:');\n",
    "[print(path) for path in paths_allStat];\n",
    "print('');\n",
    "print(f'paths to all ops files:');\n",
    "[print(path) for path in paths_allOps];\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "645ea98d",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d8f7143",
   "metadata": {},
   "source": [
    "**Important parameters**:\n",
    "- `um_per_pixel` (float):\n",
    "    - Resolution. 'micrometers per pixel' of the imaging field of view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011b19fa-0d3d-4f31-8188-4b14e84ca921",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = roicat.data_importing.Data_suite2p(\n",
    "    paths_statFiles=paths_allStat[:],\n",
    "    paths_opsFiles=paths_allOps[:],\n",
    "    um_per_pixel=2.5,  ## IMPORTANT PARAMETER\n",
    "    new_or_old_suite2p='new',\n",
    "    type_meanImg='meanImgE',\n",
    "#     FOV_images=FOVs_mixed,\n",
    "\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "assert data.check_completeness(verbose=False)['tracking'], f\"Data object is missing attributes necessary for tracking.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5e42b9-44c9-4c93-aeed-565e526fe4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "roicat.visualization.display_toggle_image_stack(data.FOV_images)\n",
    "roicat.visualization.display_toggle_image_stack(data.get_maxIntensityProjection_spatialFootprints(), clim=[0,1])\n",
    "roicat.visualization.display_toggle_image_stack(np.concatenate(data.ROI_images, axis=0)[:5000], image_size=(200,200))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6657766",
   "metadata": {},
   "source": [
    "# Alignment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91d7e32c",
   "metadata": {},
   "source": [
    "This is the most important step in the pipeline to stop and check that everything looks okay and tune parameters if necessary. I strongly recommend reading the documentation for the methods for the `roicat.tracking.alignment.Aligner` class at each step.\n",
    "\n",
    "Alignment is 4 steps:\n",
    "\n",
    "1. FOV_image augmentation\n",
    "2. Fit geometric transformation\n",
    "3. Fit non-rigid transformation (on top of the geometric)\n",
    "4. Apply transformation to ROIs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e618c2df",
   "metadata": {},
   "source": [
    "##### 1. FOV_image augmentation\n",
    "Do what is necessary to make the augmented FOV_images look good. Use the visualization tool below to help. This can include using different FOV_images, playing with the mixing factor, and playing with the CLAHE parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56ce51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aligner = roicat.tracking.alignment.Aligner(verbose=True)\n",
    "\n",
    "FOV_images = aligner.augment_FOV_images(\n",
    "    ims=data.FOV_images,\n",
    "    spatialFootprints=data.spatialFootprints,\n",
    "    roi_FOV_mixing_factor=0.5,\n",
    "    use_CLAHE=True,\n",
    "    CLAHE_grid_size=1,\n",
    "    CLAHE_clipLimit=1,\n",
    "    CLAHE_normalize=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79722a42",
   "metadata": {},
   "source": [
    "View the augmented FOV images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad16280-2da1-4ff5-a9fd-e22cb71c4065",
   "metadata": {},
   "outputs": [],
   "source": [
    "roicat.visualization.display_toggle_image_stack(FOV_images)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "658db1af",
   "metadata": {},
   "source": [
    "##### 2. Fit geometric transformation\n",
    "Play with parameters until the aligned FOV_images look good. The visualization tool below can help. Please consider reading the documentation if you have any issues with the alignment step (TODO: link to readthedocs) for more details.\n",
    "\n",
    "We like the following **important parameters**:\n",
    "- `template`=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a71c51e-dc4c-4690-9486-d2577664bbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "aligner.fit_geometric(\n",
    "#     template=FOV_images[4],\n",
    "    template=0.5,  ## specifies which image to use as the template. Either array (image), integer (ims_moving index), or float (ims_moving fractional index)\n",
    "    ims_moving=FOV_images,  ## input images\n",
    "    template_method='sequential',  ## 'sequential': align images to neighboring images (good for drifting data). 'image': align to a single image\n",
    "    mode_transform='homography',  ## type of geometric transformation. See openCV's cv2.findTransformECC for details\n",
    "    mask_borders=(50,50,50,50),  ## number of pixels to mask off the edges (top, bottom, left, right)\n",
    "    n_iter=50,  ## number of iterations for optimization\n",
    "    termination_eps=1e-09,  ## convergence tolerance\n",
    "    gaussFiltSize=31,  ## size of gaussian blurring filter applied to all images\n",
    "    auto_fix_gaussFilt_step=10,  ## increment in gaussFiltSize after a failed optimization\n",
    ")\n",
    "\n",
    "aligner.transform_images_geometric(FOV_images);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b54ce48",
   "metadata": {},
   "source": [
    "##### 3. Fit non-rigid transformation\n",
    "Play with parameters until the aligned FOV_images look good. The visualization tool below can help.\n",
    "\n",
    "We like the following **important parameters**:\n",
    "- `template`=0.5\n",
    "- `template_method`='image'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab80609-dbac-453c-9ce2-8f38ffe64a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "aligner.fit_nonrigid(\n",
    "#     template=FOV_images[1],\n",
    "    template=0.5,  ## specifies which image to use as the template. Either array (image), integer (ims_moving index), or float (ims_moving fractional index)\n",
    "    ims_moving=aligner.ims_registered_geo,  ## Input images. Typically the geometrically registered images\n",
    "    remappingIdx_init=aligner.remappingIdx_geo,  ## The remappingIdx between the original images (and ROIs) and ims_moving\n",
    "    template_method='image',  ## 'sequential': align images to neighboring images. 'image': align to a single image, good if using geometric registration first\n",
    "    mode_transform='createOptFlow_DeepFlow',  ## algorithm for non-rigid transformation. Either 'createOptFlow_DeepFlow' or 'calcOpticalFlowFarneback'. See openCV docs for each. \n",
    "    kwargs_mode_transform=None,  ## kwargs for `mode_transform`\n",
    ")\n",
    "\n",
    "aligner.transform_images_nonrigid(FOV_images);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e24834b",
   "metadata": {},
   "source": [
    "##### 4. Transform ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47f44dd-e7ed-4354-9479-ecf389a709b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "aligner.transform_ROIs(\n",
    "    ROIs=data.spatialFootprints, \n",
    "    remappingIdx=aligner.remappingIdx_nonrigid,\n",
    "    normalize=True,\n",
    ");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2054fb0a",
   "metadata": {},
   "source": [
    "Ensure that the aligned FOVs look aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11727873-69c6-4539-b15f-cf68e8216756",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Pre-alignment below')\n",
    "roicat.visualization.display_toggle_image_stack(data.FOV_images)\n",
    "print(f'Geometric alignment below')\n",
    "roicat.visualization.display_toggle_image_stack(aligner.ims_registered_geo)\n",
    "print(f'Non-rigid alignment below')\n",
    "roicat.visualization.display_toggle_image_stack(aligner.ims_registered_nonrigid)\n",
    "print(f'Transformed ROIs below')\n",
    "roicat.visualization.display_toggle_image_stack(aligner.get_ROIsAligned_maxIntensityProjection(), clim=(0, 0.05))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7ce87c7",
   "metadata": {},
   "source": [
    "# Blur ROIs\n",
    "\n",
    "ROIs from different sessions with zero spatial overlap have very low probability of being considered the same ROI. Blurring the spatial footprint masks can increase the overlap between ROIs that drift apart from each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fe242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "blurrer = roicat.tracking.blurring.ROI_Blurrer(\n",
    "    frame_shape=(data.FOV_height, data.FOV_width),  ## FOV height and width\n",
    "    kernel_halfWidth=2,  ## The half width of the 2D gaussian used to blur the ROI masks\n",
    "    plot_kernel=False,  ## Whether to visualize the 2D gaussian\n",
    ")\n",
    "\n",
    "blurrer.blur_ROIs(\n",
    "    spatialFootprints=aligner.ROIs_aligned[:],\n",
    ");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b7a9af1",
   "metadata": {},
   "source": [
    "See that the blurred ROIs are overlapping each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019a48d2-3c6a-4cc3-80e4-275c7c812c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "roicat.visualization.display_toggle_image_stack(blurrer.get_ROIsBlurred_maxIntensityProjection())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8778709",
   "metadata": {},
   "source": [
    "# ROInet embedding\n",
    "\n",
    "This step passes the images of each ROI through the ROInet neural network. The inputs are the images, the output is an array describing the visual properties of each ROI."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c6e7d80",
   "metadata": {},
   "source": [
    "Initialize the ROInet object. The `ROInet_embedder` class will automatically download and load a pretrained ROInet model. If you have a GPU, this step will be much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea35dabd-cba9-48a6-a4a0-de16f770cf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = roicat.helpers.set_device(use_GPU=True, verbose=True)\n",
    "dir_temp = tempfile.gettempdir()\n",
    "\n",
    "roinet = roicat.ROInet.ROInet_embedder(\n",
    "    device=DEVICE,  ## Which torch device to use ('cpu', 'cuda', etc.)\n",
    "    dir_networkFiles=dir_temp,  ## Directory to download the pretrained network to\n",
    "    download_method='check_local_first',  ## Check to see if a model has already been downloaded to the location (will skip if hash matches)\n",
    "    download_url='https://osf.io/x3fd2/download',  ## URL of the model\n",
    "    download_hash='7a5fb8ad94b110037785a46b9463ea94',  ## Hash of the model file\n",
    "    forward_pass_version='latent',  ## How the data is passed through the network\n",
    "    verbose=True,  ## Whether to print updates\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "395651ed",
   "metadata": {},
   "source": [
    "Resize ROIs and prepare a dataloader.\n",
    "\n",
    "**Important parameters**:\n",
    "- `um_per_pixel`: (same as specified in `data` object). Resolution of FOV. This is used to resize the ROIs to be relatively consistent across resolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c175c36-a1e0-41dc-8633-1051f2c98eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "roinet.generate_dataloader(\n",
    "    ROI_images=data.ROI_images,  ## Input images of ROIs\n",
    "    um_per_pixel=data.um_per_pixel,  ## Resolution of FOV\n",
    "    pref_plot=False,  ## Whether or not to plot the ROI sizes\n",
    "    \n",
    "    jit_script_transforms=False,  ## (advanced) Whether or not to use torch.jit.script to speed things up\n",
    "    \n",
    "    batchSize_dataloader=8,  ## (advanced) PyTorch dataloader batch_size\n",
    "    pinMemory_dataloader=True,  ## (advanced) PyTorch dataloader pin_memory\n",
    "    numWorkers_dataloader=mp.cpu_count(),  ## (advanced) PyTorch dataloader num_workers\n",
    "    persistentWorkers_dataloader=True,  ## (advanced) PyTorch dataloader persistent_workers\n",
    "    prefetchFactor_dataloader=2,  ## (advanced) PyTorch dataloader prefetch_factor\n",
    ");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14df35f7",
   "metadata": {},
   "source": [
    "In general, you want to see that a neuron fills roughly 25-50% of the area of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5329146-096f-4f7a-a377-b860f51d7e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "roicat.visualization.display_toggle_image_stack(roinet.ROI_images_rs[:1000], image_size=(200,200))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ff4a594",
   "metadata": {},
   "source": [
    "Pass the data through the network. Expect for large datasets (~40,000 ROIs) that this takes around 15 minutes on CPU or 1 minute on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0904de23-413b-4014-98db-a9724cb122c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "roinet.generate_latents();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a26152bd",
   "metadata": {},
   "source": [
    "# Scattering wavelet embedding\n",
    "\n",
    "This is similar to the ROInet embedding in purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f806b1cd-bf59-4cfb-9741-7c2c1269b9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "swt = roicat.tracking.scatteringWaveletTransformer.SWT(\n",
    "    kwargs_Scattering2D={'J': 2, 'L': 12},  ## 'J' is the number of convolutional layers. 'L' is the number of wavelet angles.\n",
    "    image_shape=data.ROI_images[0].shape[1:3],  ## size of a cropped ROI image\n",
    "    device=DEVICE,  ## PyTorch device\n",
    ")\n",
    "\n",
    "swt.transform(\n",
    "    ROI_images=roinet.ROI_images_rs,  ## All the cropped and resized ROI images\n",
    "    batch_size=100,  ## Batch size for each iteration (smaller is less memory but slower)\n",
    ");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bdbc0d2b",
   "metadata": {},
   "source": [
    "# Compute similarities\n",
    "\n",
    "Now we can compare the similarities of the ROIs. This includes calculating 4 kinds of similarities:\n",
    "1. `s_sf`: 'similarity spatial footprint'. The physical overlap between ROIs.\n",
    "2. `s_NN`: 'similarity neural network'. The similarities of the embeddings out of ROInet.\n",
    "3. `s_SWT`: 'similarity scaterring wavelet transform'. The similarities of the embeddings out of the scattering wavelet transformer.\n",
    "4. `s_sesh`: 'similarity sessions'. 0 if from the same session, 1 if from different sessions. ROIs from the same session have 0 probability of being the same.\n",
    "\n",
    "The result of this step will be a set of pairwise similarity matrices."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be787416",
   "metadata": {},
   "source": [
    "Initialize the `ROI_graph` class and compute similarities.\n",
    "To make computation more efficient, only ROIs within the same 'block' are compared against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc7d10d-7ca6-499b-80ad-0a48d3fd72e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = roicat.tracking.similarity_graph.ROI_graph(\n",
    "    n_workers=-1,  ## Number of CPU cores to use. -1 for all.\n",
    "    frame_height=data.FOV_height,\n",
    "    frame_width=data.FOV_width,\n",
    "    block_height=128,  ## size of a block\n",
    "    block_width=128,  ## size of a block\n",
    "    algorithm_nearestNeigbors_spatialFootprints='brute',  ## algorithm used to find the pairwise similarity for s_sf. ('brute' is slow but exact. See docs for others.)\n",
    "    verbose=True,  ## Whether to print outputs\n",
    ")\n",
    "\n",
    "sim.visualize_blocks()\n",
    "\n",
    "s_sf, s_NN, s_SWT, s_sesh = sim.compute_similarity_blockwise(\n",
    "    spatialFootprints=blurrer.ROIs_blurred,  ## Mask spatial footprints\n",
    "    features_NN=roinet.latents,  ## ROInet output latents\n",
    "    features_SWT=swt.latents,  ## Scattering wavelet transform output latents\n",
    "    ROI_session_bool=data.session_bool,  ## Boolean array of which ROIs belong to which sessions\n",
    "    spatialFootprint_maskPower=1.0,  ##  An exponent to raise the spatial footprints to to care more or less about bright pixels\n",
    ");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3861019",
   "metadata": {},
   "source": [
    "It is useful to normalize the similarity matrices using the local ROIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445048fd-b2ee-43f5-b81b-ebec242c9779",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.make_normalized_similarities(\n",
    "    centers_of_mass=data.centroids,  ## ROI centroid positions\n",
    "    features_NN=roinet.latents,  ## ROInet latents\n",
    "    features_SWT=swt.latents,  ## SWT latents\n",
    "    k_max=data.n_sessions*100,  ## Maximum number of nearest neighbors to consider for the normalizing distribution\n",
    "    k_min=data.n_sessions*10,  ## Minimum number of nearest neighbors to consider for the normalizing distribution\n",
    "    algo_NN='kd_tree',  ## Nearest neighbors algorithm to use\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47b02deb",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "This step does the following:\n",
    "1. Mix the similarity matrices into a single distance matrix\n",
    "2. Prune the distance matrix to remove low probability connections\n",
    "3. Perform clustering\n",
    "4. Compute quality metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd14315d",
   "metadata": {},
   "source": [
    "##### 1. Mix the similarity matrices into a single distance matrix\n",
    "\n",
    "This step finds the optimal parameters to mix the similarity matrices by tuning mixing parameters to maximally separate the distributions of pairwise similarities for ROI pairs known to be different and ROI pairs that are likely matched.\n",
    "\n",
    "The details of how this is done:\n",
    "1. Mix all the similarity matrices (`s_sf`, `s_NN_z`, `s_SWT_z`, `s_sesh`) into a single similarity matrix (`sConj`) by passing them through sigmoid functions and then taking the element-wise p-norm. This bounds the values between 0-1. \n",
    "2. Convert `sConj` into a distance matrix `dConj`.\n",
    "3. Define distribution of known 'different' ROIs as all the pairwise similarities of ROIs from the same sessions. Define the distribution of likely 'matched' ROIs as the 'different' distribution minus the distribution of 'all' pairwise similarities.\n",
    "4. Define the objective function to minimize as the sum of the overlap between the 'same' and 'different' distributions, and optimize the mixing parameters (sigmoid function and p-norm parameters) over this objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fc1ff7-1379-4d49-826f-ac22e188d7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the clusterer object by passing the similarity matrices in\n",
    "clusterer = roicat.tracking.clustering.Clusterer(\n",
    "    s_sf=sim.s_sf,\n",
    "    s_NN_z=sim.s_NN_z,\n",
    "    s_SWT_z=sim.s_SWT_z,\n",
    "    s_sesh=sim.s_sesh,\n",
    ")\n",
    "\n",
    "kwargs_makeConjunctiveDistanceMatrix_best = clusterer.find_optimal_parameters_for_pruning(\n",
    "    n_bins=None,  ## Number of bins to use for the histograms of the distributions\n",
    "    smoothing_window_bins=None,  ## Number of bins to use to smooth the distributions\n",
    "    kwargs_findParameters={\n",
    "        'n_patience': 300,  ## Number of optimization epoch to wait for tol_frac to converge\n",
    "        'tol_frac': 0.001,  ## Fractional change below which optimization will conclude\n",
    "        'max_trials': 1200,  ## Max number of optimization epochs\n",
    "        'max_duration': 60*10,  ## Max amount of time (in seconds) to allow optimization to proceed for\n",
    "    },\n",
    "    bounds_findParameters={\n",
    "        'power_NN': (0., 5.),  ## Bounds for the exponent applied to s_NN\n",
    "        'power_SWT': (0., 5.),  ## Bounds for the exponent applied to s_SWT\n",
    "        'p_norm': (-5, 0),  ## Bounds for the p-norm p value (Minkowski) applied to mix the matrices\n",
    "        'sig_NN_kwargs_mu': (0., 1.0),  ## Bounds for the sigmoid center for s_NN\n",
    "        'sig_NN_kwargs_b': (0.00, 1.5),  ## Bounds for the sigmoid slope for s_NN\n",
    "        'sig_SWT_kwargs_mu': (0., 1.0),  ## Bounds for the sigmoid center for s_SWT\n",
    "        'sig_SWT_kwargs_b': (0.00, 1.5),  ## Bounds for the sigmoid slope for s_SWT\n",
    "    },\n",
    "    n_jobs_findParameters=-1,  ## Number of CPU cores to use (-1 is all cores)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af66e1d-bbaa-48b4-992d-c1662d9ead68",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs_mcdm_tmp = kwargs_makeConjunctiveDistanceMatrix_best  ## Use the optimized parameters\n",
    "\n",
    "### Uncomment below to manually specify mixing parameters\n",
    "# kwargs_mcdm_tmp = {\n",
    "#     'power_SF': 1.0,\n",
    "#     'power_NN': 1.0,\n",
    "#     'power_SWT': 0.0,\n",
    "#     'p_norm': -4.0,\n",
    "# #         'sig_SF_kwargs': {'mu':0.5, 'b':1.0},\n",
    "#     'sig_SF_kwargs': None,\n",
    "#         'sig_NN_kwargs': {'mu':1.0, 'b':0.5},\n",
    "# #     'sig_NN_kwargs': None,\n",
    "# #         'sig_SWT_kwargs': {'mu':0.5, 'b':1.0},\n",
    "#     'sig_SWT_kwargs': None,\n",
    "# }\n",
    "\n",
    "clusterer.plot_distSame(kwargs_makeConjunctiveDistanceMatrix=kwargs_mcdm_tmp)\n",
    "\n",
    "clusterer.plot_similarity_relationships(\n",
    "    plots_to_show=[1,2,3], \n",
    "    max_samples=100000,  ## Make smaller if it is running too slow\n",
    "    kwargs_scatter={'s':1, 'alpha':0.2},\n",
    "    kwargs_makeConjunctiveDistanceMatrix=kwargs_mcdm_tmp\n",
    ");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0a19d6c",
   "metadata": {},
   "source": [
    "##### 2. Prune the distance matrix\n",
    "\n",
    "We can remove all connections in the distance graph with probabilities of connection of less than 50%. We estimate this cutoff distance as the cross-over point between the 'same' and 'different' distributions.\n",
    "\n",
    "**Important parameter**\\\n",
    "`stringency`: This value changes the threshold for pruning the distance matrix. A higher value will result in less pruning, and a lower value will result in more pruning. The value will be multiplied by the inferred threshold to get the new one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef7ab14-cadf-4f64-899a-00980e6bee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer.make_pruned_similarity_graphs(\n",
    "    d_cutoff=None,  ## Optionally manually specify a distance cutoff\n",
    "    kwargs_makeConjunctiveDistanceMatrix=kwargs_mcdm_tmp,\n",
    "    stringency=1.0,  ## \n",
    "    convert_to_probability=False,    \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56f87265",
   "metadata": {},
   "source": [
    "##### 3. Cluster\n",
    "\n",
    "There are two methods for clustering: The standard method `.fit` which is based on HDBSCAN, and `.fit_sequentialHungarian` which is an algorithm that is also used by CaImAn based on the Hungarian algorithm. The standard method takes 1-20 minutes and works better when there are many sessions, the Hungarian method takes seconds and works better when there are fewer sessions (<8).\n",
    "\n",
    "**Important parameters**:\n",
    "- For standard **`.fit`** method:\n",
    "1. `min_cluster_size`: If you only want ROIs clusters with at least a certain number of samples, specify here.\n",
    "2. `n_iter_violationCorrection`: This parameter controls how fast this step takes. Turning it down has mild effects on quality. We use around ***6***.\n",
    "\n",
    "- For **`.fit_sequentialHungarian`** method:\n",
    "1. `thresh_cost`: Determines the threshold of how distant two ROIs can be and still be matched. Smaller value is more stringent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2729208f-d551-4509-922e-8649afcb90b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data.n_sessions >= 8:\n",
    "    labels = clusterer.fit(\n",
    "        d_conj=clusterer.dConj_pruned,  ## Input distance matrix\n",
    "        session_bool=data.session_bool,  ## Boolean array of which ROIs belong to which sessions\n",
    "        min_cluster_size=2,  ## Minimum number of ROIs that can be considered a 'cluster'\n",
    "        n_iter_violationCorrection=6,  ## Number of times to redo clustering sweep after removing violations\n",
    "        split_intraSession_clusters=True,  ## Whether or not to split clusters with ROIs from the same session\n",
    "        cluster_selection_method='leaf',  ## (advanced) Method of cluster selection for HDBSCAN (see hdbscan documentation)\n",
    "        d_clusterMerge=None,  ## Distance below which all ROIs are merged into a cluster\n",
    "        alpha=0.999,  ## (advanced) Scalar applied to distance matrix in HDBSCAN (see hdbscan documentation)\n",
    "        discard_failed_pruning=True,  ## (advanced) Whether or not to set all ROIs that could be separated from clusters with ROIs from the same sessions to label=-1\n",
    "        n_steps_clusterSplit=100,  ## (advanced) How finely to step through distances to remove violations\n",
    "    )\n",
    "\n",
    "else:\n",
    "    labels = clusterer.fit_sequentialHungarian(\n",
    "        d_conj=clusterer.dConj_pruned,  ## Input distance matrix\n",
    "        session_bool=data.session_bool,  ## Boolean array of which ROIs belong to which sessions\n",
    "        thresh_cost=0.6,  ## Threshold \n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "106399e4",
   "metadata": {},
   "source": [
    "##### 4. Quality metrics\n",
    "\n",
    "Compute various quality scores for each cluster and each ROI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc074a72-b6a4-4899-9321-ef97731724e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SKIP THIS STEP FOR VERY LARGE DATASETS\n",
    "quality_metrics = clusterer.compute_quality_metrics();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "164f8c1c",
   "metadata": {},
   "source": [
    "## Collect results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88f9234c",
   "metadata": {},
   "source": [
    "1. Make different versions of the labels for convenience.\n",
    "2. Put all the useful results and info into a dictionary to save later\n",
    "3. Put all the class objects from the run into a dictionary to save later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd22cf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_squeezed, labels_bySession, labels_bool, labels_bool_bySession, labels_dict = roicat.tracking.clustering.make_label_variants(labels=labels, n_roi_bySession=data.n_roi)\n",
    "\n",
    "results = {\n",
    "    \"clusters\":{\n",
    "        \"labels\": labels_squeezed,\n",
    "        \"labels_bySession\": labels_bySession,\n",
    "        \"labels_bool\": labels_bool,\n",
    "        \"labels_bool_bySession\": labels_bool_bySession,\n",
    "        \"labels_dict\": labels_dict,\n",
    "    },\n",
    "    \"ROIs\": {\n",
    "        \"ROIs_aligned\": aligner.ROIs_aligned,\n",
    "        \"ROIs_raw\": data.spatialFootprints,\n",
    "        \"frame_height\": data.FOV_height,\n",
    "        \"frame_width\": data.FOV_width,\n",
    "        \"idx_roi_session\": np.where(data.session_bool)[1],\n",
    "        \"n_sessions\": data.n_sessions,\n",
    "    },\n",
    "    \"input_data\": {\n",
    "        \"paths_stat\": data.paths_stat,\n",
    "        \"paths_ops\": data.paths_ops,\n",
    "    },\n",
    "    \"quality_metrics\": clusterer.quality_metrics if hasattr(clusterer, 'quality_metrics') else None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb18325",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data = copy.deepcopy({\n",
    "    'data': data.serializable_dict,\n",
    "    'aligner': aligner.serializable_dict,\n",
    "    'blurrer': blurrer.serializable_dict,\n",
    "    'roinet': roinet.serializable_dict,\n",
    "    'swt': swt.serializable_dict,\n",
    "    'sim': sim.serializable_dict,\n",
    "    'clusterer': clusterer.serializable_dict,\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f000941",
   "metadata": {},
   "source": [
    "# Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bb3272",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of clusters: {len(np.unique(results[\"clusters\"][\"labels\"]))}')\n",
    "print(f'Number of discarded ROIs: {(results[\"clusters\"][\"labels\"]==-1).sum()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe7e812b",
   "metadata": {},
   "source": [
    "Look at some of the distributions of the quality metrics.\n",
    "- Silhouette score is a particularly useful one for this type of clustering. Learn more here: https://en.wikipedia.org/wiki/Silhouette_(clustering)\n",
    "- We also define a handy 'confidence' variable which is a nice heuristic you can use for thresholding for inclusion criteria\n",
    "- Note that the `sample_silhouette` score is a per-sample (per-ROI) score. So it can actually be used to remove / subselect ROIs from clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2b6166-7231-4641-972a-b4984f2cb07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = (((results['quality_metrics']['cluster_silhouette'] + 1) / 2) * results['quality_metrics']['cluster_intra_means'])\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(15,7))\n",
    "\n",
    "axs[0,0].hist(results['quality_metrics']['cluster_silhouette'], 50);\n",
    "axs[0,0].set_xlabel('cluster_silhouette');\n",
    "axs[0,0].set_ylabel('cluster counts');\n",
    "\n",
    "axs[0,1].hist(results['quality_metrics']['cluster_intra_means'], 50);\n",
    "axs[0,1].set_xlabel('cluster_intra_means');\n",
    "axs[0,1].set_ylabel('cluster counts');\n",
    "\n",
    "axs[1,0].hist(confidence, 50);\n",
    "axs[1,0].set_xlabel('confidence');\n",
    "axs[1,0].set_ylabel('cluster counts');\n",
    "\n",
    "axs[1,1].hist(results['quality_metrics']['sample_silhouette'], 50);\n",
    "axs[1,1].set_xlabel('sample_silhouette score');\n",
    "axs[1,1].set_ylabel('roi sample counts');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dc3e4d-aacc-4ef9-8d15-ff963e7067cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, counts = np.unique(results['clusters']['labels'], return_counts=True)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(counts, results['ROIs']['n_sessions']*2 + 1, range=(0, results['ROIs']['n_sessions']+1));\n",
    "plt.xlabel('n_sessions'), plt.ylabel('cluster counts');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e027846e",
   "metadata": {},
   "source": [
    "Look at a color visualization of the results. ROIs of the same color are considered a part of the same cluster. The colors are assigned randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe239a9-3e39-4990-812b-72a9f8c4c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOV_clusters = roicat.visualization.compute_colored_FOV(\n",
    "    spatialFootprints=[r.power(0.7) for r in results['ROIs']['ROIs_aligned']],  ## Spatial footprint sparse arrays\n",
    "    FOV_height=results['ROIs']['frame_height'],\n",
    "    FOV_width=results['ROIs']['frame_width'],\n",
    "    labels=results[\"clusters\"][\"labels_bySession\"],  ## cluster labels\n",
    "    # alphas_labels=confidence*1.5,  ## Set brightness of each cluster based on some 1-D array\n",
    "    alphas_labels=(clusterer.quality_metrics['cluster_silhouette'] > 0) * (clusterer.quality_metrics['cluster_intra_means'] > 0.4),\n",
    "#     alphas_sf=clusterer.quality_metrics['sample_silhouette'],  ## Set brightness of each ROI based on some 1-D array\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bf1634-f14e-42b1-87e7-df3f80207833",
   "metadata": {},
   "outputs": [],
   "source": [
    "roicat.visualization.display_toggle_image_stack(\n",
    "    FOV_clusters, \n",
    "    image_size=(np.array(FOV_clusters[0].shape)*1.5).astype(int)[:2],\n",
    "#     clim=[0,1.0],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e2ee60f",
   "metadata": {},
   "source": [
    "Visualize the images of ROIs from the same cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17211ebd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "roicat.visualization.display_cropped_cluster_ims(\n",
    "    spatialFootprints=results['ROIs']['ROIs_aligned'],\n",
    "    labels=results[\"clusters\"][\"labels\"],\n",
    "    FOV_height=results['ROIs']['frame_height'],\n",
    "    FOV_width=results['ROIs']['frame_width'],\n",
    "    n_labels_to_display=100,    \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f79a0c6",
   "metadata": {},
   "source": [
    "# Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52333d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_save = Path('/media/rich/bigSSD/analysis_data/face_rhythm/mouse_0403L').resolve()\n",
    "name_save = Path(dir_allOuterFolders).resolve().name\n",
    "\n",
    "path_save = dir_save / (name_save + '.ROICaT.tracking.results' + '.pkl')\n",
    "print(f'path_save: {path_save}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225b9fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "roicat.helpers.pickle_save(\n",
    "    obj=results,\n",
    "    filepath=path_save,\n",
    "    mkdir=True,\n",
    ")\n",
    "\n",
    "roicat.helpers.pickle_save(\n",
    "    obj=run_data,\n",
    "    filepath=str(dir_save / (name_save + '.ROICaT.tracking.rundata' + '.pkl')),\n",
    "    mkdir=True,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85c4ec85-a14a-43c2-be9b-b6a99e47cc18",
   "metadata": {},
   "source": [
    "Optionally save the FOV_clusters images as a GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f27b17a-d57c-4dcf-9bd6-d9565e0d147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOV_clusters_text = roicat.helpers.add_text_to_images(\n",
    "    (np.stack(FOV_clusters, axis=0)*255).astype(np.uint8)[:,:,:,:], \n",
    "    [[f'{ii}'] for ii in range(len(FOV_clusters))], \n",
    "    position=(20, 60),\n",
    "    font_size=2,\n",
    "    color=(255, 255, 255),\n",
    "    line_width=4,\n",
    "    font=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23ba56b-d63e-490f-aaee-1f55133f02ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOV_clusters_text_rs = roicat.helpers.resize_images(\n",
    "#     images=FOV_clusters_text, \n",
    "#     new_shape=np.array(FOV_clusters_text.shape[1:3])//2,\n",
    "#     interpolation='bilinear',\n",
    "#     antialias=False,\n",
    "#     align_corners=False,\n",
    "#     device='cpu',\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2e8caa-bf9f-4e68-9595-b812cc270250",
   "metadata": {},
   "outputs": [],
   "source": [
    "roicat.helpers.save_gif(\n",
    "    # array=FOV_clusters, \n",
    "    array=FOV_clusters_text, \n",
    "    path=str(Path(dir_save).resolve() / 'FOV_clusters_text.gif'),\n",
    "    frameRate=5.0,\n",
    "    loop=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259490ec-5a43-49cb-94c1-69ed8e8c571f",
   "metadata": {},
   "outputs": [],
   "source": [
    "roicat.helpers.save_gif(\n",
    "    array=FOV_clusters, \n",
    "    path='/home/rich/Desktop/FOV_clusters.gif',\n",
    "    frameRate=10.0,\n",
    "    loop=0,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81150d57",
   "metadata": {},
   "source": [
    "Optionally save results as a matlab file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f48da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# roicat.helpers.matlab_save(\n",
    "#     obj=results,\n",
    "#     filepath='/home/rich/Desktop/results.mat',\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112cfe73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
