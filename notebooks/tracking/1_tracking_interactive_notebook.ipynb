{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f643d500",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61fc951",
   "metadata": {},
   "source": [
    "Welcome to the interactive tracking notebook!\\\n",
    "This notebook goes through each step and allows you to tune parameters and view how it changes the results.\n",
    "\n",
    "The notebook proceeds as follows:\n",
    "1. **Import** libraries\n",
    "2. Define **paths** to data\n",
    "3. Run data through the **pipeline**. Each step of the pipeline is run by a single unique python class.\n",
    "4. **Visualize** results\n",
    "5. **Save** results\n",
    "\n",
    "As you go through the notebook, take note of the small number of parameters that are mentioned as **'important parameters'** (consider searching for these in the notebook). We consider these to be the only parameters that can have a large effect on the run output. Other parameters matter and should be considered as well, but are less critical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba945b86",
   "metadata": {},
   "source": [
    "##### If running on google colab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900d6343",
   "metadata": {},
   "source": [
    "- install roicat\n",
    "\n",
    "After running the cell below, the runtime will restart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e99d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install `roicat` if on colab\n",
    "using_colab = 'google.colab' in str(get_ipython())\n",
    "\n",
    "if using_colab:\n",
    "    !pip uninstall -y tensorflow\n",
    "    !pip install roicat[tracking]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222a013b",
   "metadata": {},
   "source": [
    "- mount google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9793b4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title mount gdrive if on colab\n",
    "#@markdown Upload your data to Google Drive, then mount the drive and access the cloud directory here.\n",
    "#@markdown You can use the sidebar to the left to browse your google drive directories.\n",
    "\n",
    "using_colab = 'google.colab' in str(get_ipython())\n",
    "\n",
    "if using_colab:\n",
    "    from google.colab import drive\n",
    "    path_gdrive = '/content/gdrive'\n",
    "    drive.mount(path_gdrive, force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f280a589",
   "metadata": {},
   "source": [
    "- enable widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b221ee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if using_colab:\n",
    "    from google.colab import output\n",
    "    output.enable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50622a4e",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9293d82",
   "metadata": {},
   "source": [
    "widen the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c53190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# widen jupyter notebook window\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:95% !important; }</style>\"))\n",
    "display(HTML(\"<style>:root { --jp-notebook-max-width: 100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cbc7a4",
   "metadata": {},
   "source": [
    "Import basic libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eb1fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import multiprocessing as mp\n",
    "import tempfile\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f24c78a",
   "metadata": {},
   "source": [
    "Import `roicat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7f4f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import roicat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8adf6d9",
   "metadata": {},
   "source": [
    "# Find paths to data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8437c9",
   "metadata": {},
   "source": [
    "In this example we are using suite2p output files, but other data types can be used (CaImAn, etc.) \\\n",
    "See the notebook on ingesting diverse data: https://github.com/RichieHakim/ROICaT/blob/main/notebooks/jupyter/other/demo_data_importing.ipynb\n",
    "\n",
    "Make a list containing the paths to all the input files.\n",
    "\n",
    "In this example we are using suite2p, so the following are defined:\n",
    "1. `paths_allStat`: a list to all the stat.npy files\n",
    "2. `paths_allOps`: a list with ops.npy files that correspond 1-to-1 with the stat.npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de208487-2702-4bd7-96be-4e62e8948de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_allOuterFolders = r'/media/rich/path/to/your/data'\n",
    "\n",
    "pathSuffixToStat = 'stat.npy'\n",
    "pathSuffixToOps = 'ops.npy'\n",
    "\n",
    "paths_allStat = roicat.helpers.find_paths(\n",
    "    dir_outer=dir_allOuterFolders,\n",
    "    reMatch=pathSuffixToStat,\n",
    "    depth=10,\n",
    ")[:]\n",
    "paths_allOps  = [str(Path(path).resolve().parent / pathSuffixToOps) for path in paths_allStat]\n",
    "\n",
    "print(f'paths to all stat files:');\n",
    "[print(path) for path in paths_allStat];\n",
    "print('');\n",
    "print(f'paths to all ops files:');\n",
    "[print(path) for path in paths_allOps];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645ea98d",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8f7143",
   "metadata": {},
   "source": [
    "**Important parameters**:\n",
    "- `um_per_pixel` (float):\n",
    "    - Resolution. 'micrometers per pixel' of the imaging field of view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19a92de-f36e-4d89-ac5c-65394d362d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = roicat.data_importing.Data_suite2p(\n",
    "    paths_statFiles=paths_allStat[:],\n",
    "    paths_opsFiles=paths_allOps[:],\n",
    "    um_per_pixel=1.0,  ## IMPORTANT PARAMETER. Use a list of floats if values differ in each session.\n",
    "    new_or_old_suite2p='new',\n",
    "    type_meanImg='meanImgE',\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "assert data.check_completeness(verbose=False)['tracking'], f\"Data object is missing attributes necessary for tracking.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35815d02",
   "metadata": {},
   "source": [
    "**Note on subselecting ROIs**:\n",
    "\n",
    "Generally, I recommend the following:\n",
    "- Do not subselecting ROIs prior to tracking. Use all available ROIs for the entire tracking process.\n",
    "- Then, after tracking, subsequently apply classification and inclusion criteria to remove bad ROIs. See the 'tracking_handling_outputs.ipynb' notebook for details.\n",
    "\n",
    "However, if you want to subselect ROIs before tracking you can call `data.remove_ROIs_by_classLabel()`. Prior to doing this, you will first need to set the class labels in the `data` object by either calling `data.set_class_labels()` or providing the `paths_iscell` input argument (if using the `Data_suite2p` class). Code below:\n",
    "\n",
    "```python\n",
    "data.remove_rois_by_classLabel(classLabel_to_keep=1, verbose=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384fe236-4e97-4cfe-8df5-16a99ed51acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "roicat.visualization.display_toggle_image_stack(data.FOV_images)\n",
    "roicat.visualization.display_toggle_image_stack(data.get_maxIntensityProjection_spatialFootprints(), clim=[0,1])\n",
    "roicat.visualization.display_toggle_image_stack(np.concatenate(data.ROI_images, axis=0)[:5000], image_size=(200,200))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f828ec1",
   "metadata": {},
   "source": [
    "### Set the device to run on\n",
    "\n",
    "If you have a GPU, some steps can be sped up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb306da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = roicat.helpers.set_device(use_GPU=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cc7c51",
   "metadata": {},
   "source": [
    "### Set determinism and seed\n",
    "Perfect determinism is not possible for a variety of reasons, but by setting `deterministic` to True you can get close by forcing the backend code to use deterministic algorithms where possible. It is generally not recommended to use deterministic algorithms and seeds due to the potential for reproducing low probability outputs and slower computation. Though, it can be useful in recreating bugs. If you want to do so, you can set the `deterministic` parameter to `True` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f52edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = roicat.util.set_random_seed(seed=None, deterministic=False)  ## Deterministic algorithms have issues, but are useful for debugging, testing, and reproducing results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6657766",
   "metadata": {},
   "source": [
    "# Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d7e32c",
   "metadata": {},
   "source": [
    "This is the most important step in the pipeline to stop and check that everything looks okay and tune parameters if necessary.\n",
    "\n",
    "Alignment is 4 steps:\n",
    "\n",
    "1. FOV_image augmentation\n",
    "2. Fit geometric transformation\n",
    "3. Fit non-rigid transformation (on top of the geometric)\n",
    "4. Apply transformation to ROIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e618c2df",
   "metadata": {},
   "source": [
    "##### 1. FOV_image augmentation\n",
    "Do what is necessary to make the augmented FOV_images look good. Use the visualization tool below to help. This can include playing with the mixing factor, normalization, and playing with the CLAHE parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56ce51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aligner = roicat.tracking.alignment.Aligner(\n",
    "    use_match_search=True,  ## Use our algorithm for doing all-pairs matching if template matching fails.\n",
    "    all_to_all=False,  ## Force the use of our algorithm for all-pairs matching. Much slower (False: O(N) vs. True: O(N^2)), but more accurate.\n",
    "    radius_in=4,  ## IMPORTANT PARAMETER: Value in micrometers used to define the maximum shift/offset between two images that are considered to be aligned. Larger means more lenient alignment.\n",
    "    radius_out=20,  ## Value in micrometers used to define the minimum shift/offset between two images that are considered to be misaligned.\n",
    "    z_threshold=4.0,  ## IMPORTANT PARAMETER: Z-score required to define two images as aligned. Larger values results in more stringent alignment requirements.\n",
    "    um_per_pixel=data.um_per_pixel[0],  ## Single value for um_per_pixel. data.um_per_pixel is typically a list of floats, so index out just one value.\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79f99e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOV_images = aligner.augment_FOV_images(\n",
    "    FOV_images=data.FOV_images,\n",
    "    spatialFootprints=data.spatialFootprints,\n",
    "    normalize_FOV_intensities=True,\n",
    "    roi_FOV_mixing_factor=0.5,\n",
    "    use_CLAHE=True,  ## IMPORTANT PARAMETER. Use Set to False if data is poor quality or poorly aligned.\n",
    "    CLAHE_grid_block_size=10,  ## IMPORTANT PARAMETER. Use smaller values for higher precision but higher chance of failure.\n",
    "    CLAHE_clipLimit=1.0,\n",
    "    CLAHE_normalize=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79722a42",
   "metadata": {},
   "source": [
    "View the augmented FOV images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cd401a-d988-4ada-aac7-fecb00e82ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "roicat.visualization.display_toggle_image_stack(FOV_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658db1af",
   "metadata": {},
   "source": [
    "##### 2. Fit geometric transformation\n",
    "This is an important step. Consider reading the comments and arguments closely.\n",
    "\n",
    "Play with parameters until the aligned FOV_images look good. The visualization tool below can help.\n",
    "\n",
    "This step creates the attribute: `aligner.ims_registered_geo`, which are the registered images after the geometric transformation.\n",
    "\n",
    "We like the following **important parameters**:\n",
    "- `template=0.5`\n",
    "- `method='RoMa'` if you have a GPU, otherwise use one of the other methods\n",
    "- `template_method='sequential'` best if there are large changes that occurrs slowly over sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c506ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "aligner.fit_geometric(\n",
    "    template=0.5,  ## specifies which image to use as the template. Either array (image), integer (ims_moving index), or float (ims_moving fractional index)\n",
    "    ims_moving=FOV_images,  ## input images\n",
    "    template_method='sequential',  ## 'sequential': align images to neighboring images (good for drifting data). 'image': align to a single image\n",
    "    mask_borders=(0, 0, 0, 0),  ## number of pixels to mask off the edges (top, bottom, left, right)\n",
    "    method='DISK_LightGlue',  ## See below for options.\n",
    "    kwargs_method = {\n",
    "        'RoMa': {  ## Accuracy: Best, Speed: Very slow (can be fast with a GPU).\n",
    "            'model_type': 'outdoor',\n",
    "            'n_points': 10000,  ## Higher values mean more points are used for the registration. Useful for larger FOV_images. Larger means slower.\n",
    "            'batch_size': 1000,\n",
    "        },\n",
    "        'DISK_LightGlue': {  ## Accuracy: Good, Speed: Fast.\n",
    "            'num_features': 3000,  ## Number of features to extract and match. I've seen best results around 2048 despite higher values typically being better.\n",
    "            'threshold_confidence': 0.0,  ## Higher values means fewer but better matches.\n",
    "            'window_nms': 7,  ## Non-maximum suppression window size. Larger values mean fewer non-suppressed points.\n",
    "        },\n",
    "        'LoFTR': {  ## Accuracy: Okay. Speed: Medium.\n",
    "            'model_type': 'indoor_new',\n",
    "            'threshold_confidence': 0.2,  ## Higher values means fewer but better matches.\n",
    "        },\n",
    "        'ECC_cv2': {  ## Accuracy: Okay. Speed: Medium.\n",
    "            'mode_transform': 'euclidean',  ## Must be one of {'translation', 'affine', 'euclidean', 'homography'}. See cv2 documentation on findTransformECC for more details.\n",
    "            'n_iter': 200,\n",
    "            'termination_eps': 1e-09,  ## Termination criteria for the registration algorithm. See documentation for more details.\n",
    "            'gaussFiltSize': 1,  ## Size of the gaussian filter used to smooth the FOV_image before registration. Larger values mean more smoothing.\n",
    "            'auto_fix_gaussFilt_step': 10,  ## If the registration fails, then the gaussian filter size is reduced by this amount and the registration is tried again.\n",
    "        },\n",
    "        'PhaseCorrelation': {  ## Accuracy: Poor. Speed: Very fast. Notes: Only applicable for translations, not rotations or scaling.\n",
    "            'bandpass_freqs': [1, 30],\n",
    "            'order': 5,\n",
    "        },\n",
    "        'NullRegistration': {},  ## No registration, no warping.\n",
    "    },\n",
    "    constraint='affine',  ## Must be one of {'rigid', 'euclidean', 'similarity', 'affine', 'homography'}. Choose constraint based on expected changes in images; use the simplest constraint that is applicable.\n",
    "    kwargs_RANSAC = {\n",
    "        'inl_thresh': 3.0,  ## cv2.findHomography RANSAC inlier threshold. Larger values mean more lenient matching.\n",
    "        'max_iter': 100,\n",
    "        'confidence': 0.99,\n",
    "    },\n",
    "    verbose=True,  ## Set to 3 to view plots of the alignment process if available for the method.\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf82ce8",
   "metadata": {},
   "source": [
    "Plot the alignment scores. The '(final)' score is the alignment score between a pair of images given the final compsed geometric transformation between them. The '(direct)' score (only shown if the match search algorithm was used) is the alignment score between a pair of images given the single direct geometric transformation between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d261ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "aligner.plot_alignment_results_geometric();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b54ce48",
   "metadata": {},
   "source": [
    "##### 3. Fit non-rigid transformation\n",
    "Play with parameters until the aligned FOV_images look good. The visualization tool below can help.\n",
    "\n",
    "We like the following **important parameters**:\n",
    "- `template`=0.5\n",
    "- `template_method`='image'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcb87ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "aligner.fit_nonrigid(\n",
    "    template=0.5,  ## specifies which image to use as the template. Either array (image), integer (ims_moving index), or float (ims_moving fractional index)\n",
    "    ims_moving=aligner.ims_registered_geo,  ## Input images. Typically the geometrically registered images\n",
    "    remappingIdx_init=aligner.remappingIdx_geo,  ## The remappingIdx between the original images (and ROIs) and ims_moving\n",
    "    template_method='image',  ## 'sequential': align images to neighboring images. 'image': align to a single image, good if using geometric registration first\n",
    "    method='DeepFlow',\n",
    "    kwargs_method = {\n",
    "        'DeepFlow': {},  ## Accuracy: Good (good in middle, poor on edges), Speed: Fast (CPU only)\n",
    "        'RoMa': {  ## Accuracy: Okay (decent in middle, poor on edges), Speed: Slow (can be fast with a GPU), Notes: This method can work on the raw images without pre-registering using geometric methods.\n",
    "            'model_type': 'outdoor',\n",
    "        },\n",
    "        'OpticalFlowFarneback': {  ## Accuracy: Varies (can sometimes be tuned to be the best as there are no edge artifacts), Speed: Medium (CPU only)\n",
    "            'pyr_scale': 0.7,\n",
    "            'levels': 5,\n",
    "            'winsize': 256,\n",
    "            'iterations': 15,\n",
    "            'poly_n': 5,\n",
    "            'poly_sigma': 1.5,            \n",
    "        },\n",
    "        'NullRegistration': {},\n",
    "    },    \n",
    ")\n",
    "\n",
    "aligner.transform_images_nonrigid(FOV_images);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d93135",
   "metadata": {},
   "outputs": [],
   "source": [
    "aligner.plot_alignment_results_nonrigid();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e24834b",
   "metadata": {},
   "source": [
    "##### 4. Transform ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fbf309-6064-4539-87de-72a59d1863dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "aligner.transform_ROIs(\n",
    "    ROIs=data.spatialFootprints, \n",
    "    remappingIdx=aligner.remappingIdx_nonrigid,\n",
    "    # remappingIdx=aligner.remappingIdx_geo,\n",
    "    normalize=True,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2054fb0a",
   "metadata": {},
   "source": [
    "Ensure that the aligned FOVs look aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd09701-7df7-4af5-a8d8-1fed52c7d4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Pre-alignment below')\n",
    "roicat.visualization.display_toggle_image_stack(data.FOV_images)\n",
    "print(f'Geometric alignment below')\n",
    "roicat.visualization.display_toggle_image_stack(aligner.ims_registered_geo)\n",
    "print(f'Non-rigid alignment below')\n",
    "roicat.visualization.display_toggle_image_stack(aligner.ims_registered_nonrigid)\n",
    "print(f'Transformed ROIs below')\n",
    "roicat.visualization.display_toggle_image_stack(aligner.get_ROIsAligned_maxIntensityProjection(normalize=True), clim=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91d0c1d",
   "metadata": {},
   "source": [
    "# Blur ROIs\n",
    "\n",
    "ROIs from different sessions with zero spatial overlap have very low probability of being considered the same ROI during the clustering step. Blurring the spatial footprint masks can increase the overlap between ROIs that drift apart from each other. It's a good idea to increase the `kernel_halfWidth` if you are working with sparsely labeled ROIs or ROIs that change/move from session to session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0dd22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "blurrer = roicat.tracking.blurring.ROI_Blurrer(\n",
    "    frame_shape=(data.FOV_height, data.FOV_width),  ## FOV height and width\n",
    "    kernel_halfWidth=4,  ## The half width of the 2D gaussian used to blur the ROI masks\n",
    "    plot_kernel=False,  ## Whether to visualize the 2D gaussian\n",
    ")\n",
    "\n",
    "blurrer.blur_ROIs(\n",
    "    spatialFootprints=aligner.ROIs_aligned[:],\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a2c920",
   "metadata": {},
   "source": [
    "See that the blurred ROIs are overlapping each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faf8d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "roicat.visualization.display_toggle_image_stack(blurrer.get_ROIsBlurred_maxIntensityProjection())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcb6963",
   "metadata": {},
   "source": [
    "# ROInet embedding\n",
    "\n",
    "This step passes the images of each ROI through the ROInet neural network. The inputs are the images, the output is an array describing the visual properties of each ROI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887edd60",
   "metadata": {},
   "source": [
    "Initialize the ROInet object. The `ROInet_embedder` class will automatically download and load a pretrained ROInet model. If you have a GPU, this step will be much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb483436",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_temp = tempfile.gettempdir()\n",
    "\n",
    "roinet = roicat.ROInet.ROInet_embedder(\n",
    "    device=DEVICE,  ## Which torch device to use ('cpu', 'cuda', etc.)\n",
    "    dir_networkFiles=dir_temp,  ## Directory to download the pretrained network to\n",
    "    download_method='check_local_first',  ## Check to see if a model has already been downloaded to the location (will skip if hash matches)\n",
    "    download_url='https://osf.io/x3fd2/download',  ## URL of the model\n",
    "    download_hash='7a5fb8ad94b110037785a46b9463ea94',  ## Hash of the model file\n",
    "    forward_pass_version='latent',  ## How the data is passed through the network\n",
    "    verbose=True,  ## Whether to print updates\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75e4d6e",
   "metadata": {},
   "source": [
    "Resize ROIs and prepare a dataloader.\n",
    "\n",
    "**Important parameters**:\n",
    "- `um_per_pixel`: (same as specified in `data` object). Resolution of FOV. This is used to resize the ROIs to be relatively consistent across resolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f093d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "roinet.generate_dataloader(\n",
    "    ROI_images=data.ROI_images,  ## Input images of ROIs\n",
    "    um_per_pixel=data.um_per_pixel,  ## Resolution of FOV\n",
    "    pref_plot=False,  ## Whether or not to plot the ROI sizes\n",
    "    \n",
    "    jit_script_transforms=False,  ## (advanced) Whether or not to use torch.jit.script to speed things up\n",
    "    \n",
    "    batchSize_dataloader=8,  ## (advanced) PyTorch dataloader batch_size\n",
    "    pinMemory_dataloader=True,  ## (advanced) PyTorch dataloader pin_memory\n",
    "    numWorkers_dataloader=mp.cpu_count(),  ## (advanced) PyTorch dataloader num_workers\n",
    "    persistentWorkers_dataloader=True,  ## (advanced) PyTorch dataloader persistent_workers\n",
    "    prefetchFactor_dataloader=2,  ## (advanced) PyTorch dataloader prefetch_factor\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8251bf78",
   "metadata": {},
   "source": [
    "In general, you want to see that a neuron fills roughly 25-50% of the area of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d1aeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "roicat.visualization.display_toggle_image_stack(roinet.ROI_images_rs[:1000], image_size=(200,200))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101b8bc1",
   "metadata": {},
   "source": [
    "Pass the data through the network. Expect for large datasets (~40,000 ROIs) that this takes around 15 minutes on CPU or 1 minute on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7588277",
   "metadata": {},
   "outputs": [],
   "source": [
    "roinet.generate_latents();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26152bd",
   "metadata": {},
   "source": [
    "# Scattering wavelet embedding\n",
    "\n",
    "This is similar to the ROInet embedding in purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f806b1cd-bf59-4cfb-9741-7c2c1269b9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "swt = roicat.tracking.scatteringWaveletTransformer.SWT(\n",
    "    kwargs_Scattering2D={'J': 2, 'L': 12},  ## 'J' is the number of convolutional layers. 'L' is the number of wavelet angles.\n",
    "    image_shape=data.ROI_images[0].shape[1:3],  ## size of a cropped ROI image\n",
    "    device=DEVICE,  ## PyTorch device\n",
    ")\n",
    "\n",
    "swt.transform(\n",
    "    ROI_images=roinet.ROI_images_rs,  ## All the cropped and resized ROI images\n",
    "    batch_size=100,  ## Batch size for each iteration (smaller is less memory but slower)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbc0d2b",
   "metadata": {},
   "source": [
    "# Compute similarities\n",
    "\n",
    "Now we can compare the similarities of the ROIs. This includes calculating 4 kinds of similarities:\n",
    "1. `s_sf`: 'similarity spatial footprint'. The physical overlap between ROIs.\n",
    "2. `s_NN`: 'similarity neural network'. The similarities of the embeddings out of ROInet.\n",
    "3. `s_SWT`: 'similarity scaterring wavelet transform'. The similarities of the embeddings out of the scattering wavelet transformer.\n",
    "4. `s_sesh`: 'similarity sessions'. 0 if from the same session, 1 if from different sessions. ROIs from the same session have 0 probability of being the same.\n",
    "\n",
    "The result of this step will be a set of pairwise similarity matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be787416",
   "metadata": {},
   "source": [
    "Initialize the `ROI_graph` class and compute similarities.\n",
    "To make computation more efficient, only ROIs within the same 'block' are compared against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c5d1e5-67f7-4195-b009-7aa10cd45f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = roicat.tracking.similarity_graph.ROI_graph(\n",
    "    n_workers=-1,  ## Number of CPU cores to use. -1 for all.\n",
    "    frame_height=data.FOV_height,\n",
    "    frame_width=data.FOV_width,\n",
    "    block_height=128,  ## size of a block\n",
    "    block_width=128,  ## size of a block\n",
    "    algorithm_nearestNeigbors_spatialFootprints='brute',  ## algorithm used to find the pairwise similarity for s_sf. ('brute' is slow but exact. See docs for others.)\n",
    "    verbose=True,  ## Whether to print outputs\n",
    ")\n",
    "\n",
    "sim.visualize_blocks()\n",
    "\n",
    "s_sf, s_NN, s_SWT, s_sesh = sim.compute_similarity_blockwise(\n",
    "    spatialFootprints=blurrer.ROIs_blurred,  ## Mask spatial footprints\n",
    "    features_NN=roinet.latents,  ## ROInet output latents\n",
    "    features_SWT=swt.latents,  ## Scattering wavelet transform output latents\n",
    "    ROI_session_bool=data.session_bool,  ## Boolean array of which ROIs belong to which sessions\n",
    "    spatialFootprint_maskPower=1.0,  ##  An exponent to raise the spatial footprints to to care more or less about bright pixels\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3861019",
   "metadata": {},
   "source": [
    "It is useful to normalize the similarity matrices using the local ROIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8269de51-e506-41a9-b979-d8fb8c8bf5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.make_normalized_similarities(\n",
    "    centers_of_mass=data.centroids,  ## ROI centroid positions\n",
    "    features_NN=roinet.latents,  ## ROInet latents\n",
    "    features_SWT=swt.latents,  ## SWT latents\n",
    "    k_max=data.n_sessions*100,  ## Maximum number of nearest neighbors to consider for the normalizing distribution\n",
    "    k_min=data.n_sessions*10,  ## Minimum number of nearest neighbors to consider for the normalizing distribution\n",
    "    algo_NN='kd_tree',  ## Nearest neighbors algorithm to use\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b02deb",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "This step does the following:\n",
    "1. Mix the similarity matrices into a single distance matrix\n",
    "2. Prune the distance matrix to remove low probability connections\n",
    "3. Perform clustering\n",
    "4. Compute quality metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd14315d",
   "metadata": {},
   "source": [
    "#### 1. Mix the similarity matrices into a single distance matrix\n",
    "\n",
    "This step can be done either automatically, using the `clusterer.find_optimal_parameters_for_pruning` method, or manually by specifying the `kwargs_makeConjunctiveDistanceMatrix` dictionary. If you have a smaller total number of ROIs (<100 ROIs per session and/or <8 sessions), then it may be a good idea to manually play with the parameters in the next cell instead of using the automatic method.\n",
    "\n",
    "<br></br>\n",
    "\n",
    "##### Option A: Automatic Method\n",
    "This step finds the optimal parameters to mix the similarity matrices by tuning mixing parameters to maximally separate the distributions of pairwise similarities for ROI pairs known to be different and ROI pairs that are likely matched.\n",
    "\n",
    "Some of the details of underlying algorithm:\n",
    "1. For each step in the optimization process, all the similarity matrices (`s_sf`, `s_NN_z`, `s_SWT_z`, `s_sesh`) are each passed through a sigmoid activation function that is parameterized (e.g. `'power_SF'`, `'sig_SF_kwargs'`), then all the similarity matrices are combined using a p-norm, where the 'p' is parameterized with `'p_norm'`. This results in a single conjunctive similarity matrix called `sConj` bounded between 0-1. \n",
    "2. `sConj` is converted into a distance matrix `dConj` (bounded from 1-0).\n",
    "3. `dConj` is then passed through the objective function: The full distance matrix is separated into a few components. First, all pairs of ROIs that are known to be from 'different' sources because they are from the same session are separated out into a distribution of pairwise distances (`d_diff`). Second, we define pairs of ROIs that are likely to be from the same source as `d_same` = `d_all` - `d_diff`. The objective function is then the overlap between the `d_diff` and `d_same` distributions. \n",
    "4. The objective function is minimized by tuning the mixing parameters in `kwargs_makeConjunctiveDistanceMatrix`.\n",
    "5. The output of this step is the optimal `kwargs_makeConjunctiveDistanceMatrix` dictionary.\n",
    "\n",
    "<br></br>\n",
    "\n",
    "##### Option B: Manual Method\n",
    "You can also simply specify the `kwargs_makeConjunctiveDistanceMatrix` dictionary manually. This is useful if you have a good idea of what the optimal parameters are or if the automatic method is not working well. Uncomment the code block below to overwrite the `kwargs_mcdm_tmp` variable.\n",
    "\n",
    "<br></br>\n",
    "\n",
    "#### TROUBLESHOOTING FOR THIS STEP\n",
    "- If you have any issues, just email Rich Hakim or open an issue on the github issues page.\n",
    "- If you see: `'No crossover found, not plotting'`: Your data may not be easily separable. For some people, this is because the number of matching ROIs is very low compared to the number of non-matching ROIs. I recommend trying out the manual method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f29fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the clusterer object by passing the similarity matrices in\n",
    "clusterer = roicat.tracking.clustering.Clusterer(\n",
    "    s_sf=sim.s_sf,\n",
    "    s_NN_z=sim.s_NN_z,\n",
    "    s_SWT_z=sim.s_SWT_z,\n",
    "    s_sesh=sim.s_sesh,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cfbb6e",
   "metadata": {},
   "source": [
    "#### Automatic method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba1ff7b-d6f9-4ba5-8a01-f8851cd10907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below to automatically find mixing parameters\n",
    "kwargs_makeConjunctiveDistanceMatrix_best = clusterer.find_optimal_parameters_for_pruning(\n",
    "    n_bins=None,  ## Number of bins to use for the histograms of the distributions. If None, then a heuristic is used.\n",
    "    smoothing_window_bins=None,  ## Number of bins to use to smooth the distributions. If None, then a heuristic is used.\n",
    "    kwargs_findParameters={\n",
    "        'n_patience': 300,  ## Number of optimization epoch to wait for tol_frac to converge\n",
    "        'tol_frac': 0.001,  ## Fractional change below which optimization will conclude\n",
    "        'max_trials': 1200,  ## Max number of optimization epochs\n",
    "        'max_duration': 60*10,  ## Max amount of time (in seconds) to allow optimization to proceed for\n",
    "        'value_stop': 0.0,  ## Goal value. If value equals or goes below value_stop, optimization is stopped.\n",
    "    },\n",
    "    bounds_findParameters={\n",
    "        'power_NN': [0.0, 2.],  ## Bounds for the exponent applied to s_NN\n",
    "        'power_SWT': [0.0, 2.],  ## Bounds for the exponent applied to s_SWT\n",
    "        'p_norm': [-5, -0.1],  ## Bounds for the p-norm p value (Minkowski) applied to mix the matrices\n",
    "        'sig_NN_kwargs_mu': [0., 1.0],  ## Bounds for the sigmoid center for s_NN\n",
    "        'sig_NN_kwargs_b': [0.1, 1.5],  ## Bounds for the sigmoid slope for s_NN\n",
    "        'sig_SWT_kwargs_mu': [0., 1.0],  ## Bounds for the sigmoid center for s_SWT\n",
    "        'sig_SWT_kwargs_b': [0.1, 1.5],  ## Bounds for the sigmoid slope for s_SWT\n",
    "    },\n",
    "    n_jobs_findParameters=-1,  ## Number of CPU cores to use (-1 is all cores)\n",
    "    seed=SEED,  ## Random seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d62119",
   "metadata": {},
   "source": [
    "#### Manual method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d439825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment below to manually specify mixing parameters\n",
    "# kwargs_makeConjunctiveDistanceMatrix_best = {\n",
    "#     'power_SF': 1.0,   ## s_sf**power_SF   (Higher values means clustering is more sensitive to spatial overlap of ROIs)\n",
    "#     'power_NN': 4.988104678381475,   ## s_NN**power_NN   (Higher values means clustering is more sensitive to visual similarity of ROIs)\n",
    "#     'power_SWT': 3.2306101591115177,  ## s_SWT**power_SWT (Higher values means clustering is more sensitive to visual similarity of ROIs)\n",
    "#     'p_norm': -3.7239759637888254,    ## norm([s_sf, s_NN, s_SWT], p=p_norm) (Higher values means clustering requires all similarity metrics to be high)\n",
    "# #     'sig_SF_kwargs': {'mu':0.5, 'b':1.0},  ## Sigmoid parameters for s_sf (mu is the center, b is the slope)\n",
    "#     'sig_SF_kwargs': None,\n",
    "#     'sig_NN_kwargs': {'mu':0.022482651499435957, 'b':0.02659655318391102},    ## Sigmoid parameters for s_NN (mu is the center, b is the slope)\n",
    "# #     'sig_NN_kwargs': None,\n",
    "#     'sig_SWT_kwargs': {'mu':0.10929602726304388, 'b':0.25801625013167434}, ## Sigmoid parameters for s_SWT (mu is the center, b is the slope)\n",
    "# #     'sig_SWT_kwargs': None,\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83cd43e",
   "metadata": {},
   "source": [
    "#### View mixing results\n",
    "The goal is to see a **bimodal curve** in the pairwise similarities and a **clear cross-over point** (specified by the vertical dotted line) between 'same' and 'diff' pairs of ROIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a004e69f-e4dc-48e7-8340-e4fd6eec0716",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer.plot_distSame(kwargs_makeConjunctiveDistanceMatrix=kwargs_makeConjunctiveDistanceMatrix_best)\n",
    "\n",
    "clusterer.plot_similarity_relationships(\n",
    "    plots_to_show=[1,2,3], \n",
    "    max_samples=100000,  ## Make smaller if it is running too slow\n",
    "    kwargs_scatter={'s':1, 'alpha':0.2},\n",
    "    kwargs_makeConjunctiveDistanceMatrix=kwargs_makeConjunctiveDistanceMatrix_best\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2511098",
   "metadata": {},
   "source": [
    "##### 2. Prune the distance matrix\n",
    "\n",
    "We can remove all connections in the distance graph with probabilities of connection of less than 50%. We estimate this cutoff distance as the cross-over point between the 'same' and 'different' distributions.\n",
    "\n",
    "**Important parameter**\\\n",
    "`stringency`: This value changes the threshold for pruning the distance matrix. A higher value will result in less pruning, and a lower value will result in more pruning. The value will be multiplied by the inferred threshold to get the new one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ed79a9-b95f-4670-bf91-31ae65235898",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer.make_pruned_similarity_graphs(\n",
    "    d_cutoff=None,  ## Optionally manually specify a distance cutoff\n",
    "    kwargs_makeConjunctiveDistanceMatrix=kwargs_makeConjunctiveDistanceMatrix_best,\n",
    "    stringency=1.0,  ## Modifies the threshold for pruning the distance matrix. Higher values result in LESS pruning. New d_cutoff = stringency * truncated d_cutoff.\n",
    "    convert_to_probability=False,    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f87265",
   "metadata": {},
   "source": [
    "##### 3. Cluster\n",
    "\n",
    "There are two methods for clustering: The standard method `.fit` which is based on HDBSCAN, and `.fit_sequentialHungarian` which is an algorithm that is also used by CaImAn based on the Hungarian algorithm. The standard method takes 1-20 minutes and works better when there are many sessions, the Hungarian method takes seconds and works better when there are fewer sessions (<8).\n",
    "\n",
    "**Important parameters**:\n",
    "- For standard **`.fit`** method:\n",
    "1. `min_cluster_size`: If you only want ROIs clusters with at least a certain number of samples, specify here.\n",
    "2. `n_iter_violationCorrection`: This parameter controls how fast this step takes. Turning it down has mild effects on quality. We use around ***6***.\n",
    "\n",
    "- For **`.fit_sequentialHungarian`** method:\n",
    "1. `thresh_cost`: Determines the threshold of how distant two ROIs can be and still be matched. Smaller value is more stringent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db56330-e5e4-4400-ad7a-e63934a2eaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data.n_sessions >= 6:\n",
    "    labels = clusterer.fit(\n",
    "        d_conj=clusterer.dConj_pruned,  ## Input distance matrix\n",
    "        session_bool=data.session_bool,  ## Boolean array of which ROIs belong to which sessions\n",
    "        min_cluster_size=2,  ## Minimum number of ROIs that can be considered a 'cluster'\n",
    "        n_iter_violationCorrection=6,  ## Number of times to redo clustering sweep after removing violations\n",
    "        split_intraSession_clusters=True,  ## Whether or not to split clusters with ROIs from the same session\n",
    "        cluster_selection_method='leaf',  ## (advanced) Method of cluster selection for HDBSCAN (see hdbscan documentation)\n",
    "        d_clusterMerge=None,  ## Distance below which all ROIs are merged into a cluster\n",
    "        alpha=0.999,  ## (advanced) Scalar applied to distance matrix in HDBSCAN (see hdbscan documentation)\n",
    "        discard_failed_pruning=True,  ## (advanced) Whether or not to set all ROIs that could be separated from clusters with ROIs from the same sessions to label=-1\n",
    "        n_steps_clusterSplit=100,  ## (advanced) How finely to step through distances to remove violations\n",
    "    )\n",
    "\n",
    "else:\n",
    "    labels = clusterer.fit_sequentialHungarian(\n",
    "        d_conj=clusterer.dConj_pruned,  ## Input distance matrix\n",
    "        session_bool=data.session_bool,  ## Boolean array of which ROIs belong to which sessions\n",
    "        thresh_cost=0.8,  ## Threshold. Higher values result in more permissive clustering. Specifically, the pairwise metric distance between ROIs above which two ROIs cannot be clustered together.\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106399e4",
   "metadata": {},
   "source": [
    "##### 4. Quality metrics\n",
    "\n",
    "Compute various quality scores for each cluster and each ROI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318b4f12-1ea5-4487-9188-504991ad272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SKIP THIS STEP FOR VERY LARGE DATASETS\n",
    "quality_metrics = clusterer.compute_quality_metrics();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164f8c1c",
   "metadata": {},
   "source": [
    "## Collect results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f9234c",
   "metadata": {},
   "source": [
    "1. Make different versions of the labels for convenience.\n",
    "2. Put all the useful results and info into a dictionary to save later. ADJUST THIS ANY WAY YOU WANT.\n",
    "3. Put all the class objects from the run into a dictionary to save later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd22cf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_squeezed, labels_bySession, labels_bool, labels_bool_bySession, labels_dict = roicat.tracking.clustering.make_label_variants(labels=labels, n_roi_bySession=data.n_roi)\n",
    "\n",
    "results_clusters = {\n",
    "    'labels': labels_squeezed,\n",
    "    'labels_bySession': labels_bySession,\n",
    "    'labels_dict': labels_dict,\n",
    "    'quality_metrics': clusterer.quality_metrics if hasattr(clusterer, 'quality_metrics') else None,\n",
    "}\n",
    "\n",
    "results_all = {\n",
    "    \"clusters\":{\n",
    "        \"labels\": roicat.util.JSON_List(labels_squeezed),\n",
    "        \"labels_bySession\": roicat.util.JSON_List(labels_bySession),\n",
    "        \"labels_bool\": labels_bool,\n",
    "        \"labels_bool_bySession\": labels_bool_bySession,\n",
    "        \"labels_dict\": roicat.util.JSON_Dict(labels_dict),\n",
    "        \"quality_metrics\": roicat.util.JSON_Dict(clusterer.quality_metrics) if hasattr(clusterer, 'quality_metrics') else None,\n",
    "    },\n",
    "    \"ROIs\": {\n",
    "        \"ROIs_aligned\": aligner.ROIs_aligned,\n",
    "        \"ROIs_raw\": data.spatialFootprints,\n",
    "        \"frame_height\": data.FOV_height,\n",
    "        \"frame_width\": data.FOV_width,\n",
    "        \"idx_roi_session\": np.where(data.session_bool)[1],\n",
    "        \"n_sessions\": data.n_sessions,\n",
    "    },\n",
    "    \"input_data\": {\n",
    "        \"paths_stat\": data.paths_stat,\n",
    "        \"paths_ops\": data.paths_ops,\n",
    "    },\n",
    "}\n",
    "\n",
    "run_data = {\n",
    "    'data': data.__dict__,\n",
    "    'aligner': aligner.__dict__,\n",
    "    'blurrer': blurrer.__dict__,\n",
    "    'roinet': roinet.__dict__,\n",
    "    'swt': swt.__dict__,\n",
    "    'sim': sim.__dict__,\n",
    "    'clusterer': clusterer.__dict__,\n",
    "}\n",
    "\n",
    "params_used = {name: mod['params'] for name, mod in run_data.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f000941",
   "metadata": {},
   "source": [
    "# Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4384f2-e388-463f-9322-c7cb84948488",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of clusters: {len(np.unique(results_clusters[\"labels\"]))}')\n",
    "print(f'Number of discarded ROIs: {(np.array(results_clusters[\"labels\"])==-1).sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7e812b",
   "metadata": {},
   "source": [
    "Look at some of the distributions of the quality metrics.\n",
    "- Silhouette score is a particularly useful one for this type of clustering. Learn more here: https://en.wikipedia.org/wiki/Silhouette_(clustering)\n",
    "- We also define a handy 'confidence' variable which is a nice heuristic you can use for thresholding for inclusion criteria\n",
    "- Note that the `sample_silhouette` score is a per-sample (per-ROI) score. So it can actually be used to remove / subselect ROIs from clusters.\n",
    "\n",
    "A good rule of thumb is to use an inclusion criteria of:\n",
    "- `sample_silhouette` > 0.1\n",
    "- `cluster_silhouette` > 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d012c5-ff39-4aa0-860b-cdd4e71dc0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "roicat.tracking.clustering.plot_quality_metrics(quality_metrics=quality_metrics, labels=labels_squeezed, n_sessions=data.n_sessions);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e027846e",
   "metadata": {},
   "source": [
    "Look at a color visualization of the results. ROIs of the same color are considered a part of the same cluster. The colors are assigned randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe239a9-3e39-4990-812b-72a9f8c4c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOV_clusters = roicat.visualization.compute_colored_FOV(\n",
    "    spatialFootprints=[r.power(1.0) for r in results_all['ROIs']['ROIs_aligned']],  ## Spatial footprint sparse arrays\n",
    "    FOV_height=results_all['ROIs']['frame_height'],\n",
    "    FOV_width=results_all['ROIs']['frame_width'],\n",
    "    labels=results_all[\"clusters\"][\"labels_bySession\"],  ## cluster labels\n",
    "#     labels=(np.array(results[\"clusters\"][\"labels\"])!=-1).astype(np.int64),  ## cluster labels\n",
    "    # alphas_labels=confidence*1.5,  ## Set brightness of each cluster based on some 1-D array\n",
    "    # alphas_labels=(np.array(clusterer.quality_metrics['cluster_silhouette']) > -0.2) * (np.array(clusterer.quality_metrics['cluster_intra_means']) > 0.3),\n",
    "    # alphas_sf=np.array(clusterer.quality_metrics['sample_silhouette']) > 0,  ## Set brightness of each ROI based on some 1-D array\n",
    ")\n",
    "\n",
    "roicat.visualization.display_toggle_image_stack(\n",
    "    FOV_clusters, \n",
    "    image_size=1.5,\n",
    "#     clim=[0,1.0],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2ee60f",
   "metadata": {},
   "source": [
    "Visualize the images of ROIs from the same cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1f0831-74f3-4530-ada7-33fe281e90de",
   "metadata": {},
   "outputs": [],
   "source": [
    "roicat.visualization.display_cropped_cluster_ims(\n",
    "    spatialFootprints=results_all['ROIs']['ROIs_aligned'],\n",
    "    labels=np.array(results_all[\"clusters\"][\"labels\"]),\n",
    "    FOV_height=results_all['ROIs']['frame_height'],\n",
    "    FOV_width=results_all['ROIs']['frame_width'],\n",
    "    n_labels_to_display=10,    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f79a0c6",
   "metadata": {},
   "source": [
    "# Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3151ee3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the directory to save the results to\n",
    "dir_save = '/media/rich/bigSSD/data_tmp/test_data/'\n",
    "name_save = 'mouse_1'\n",
    "\n",
    "paths_save = {\n",
    "    'results_clusters': str(Path(dir_save) / f'{name_save}.tracking.results_clusters.json'),\n",
    "    'params_used':      str(Path(dir_save) / f'{name_save}.tracking.params_used.json'),\n",
    "    'results_all':      str(Path(dir_save) / f'{name_save}.tracking.results_all.richfile'),\n",
    "    'run_data':         str(Path(dir_save) / f'{name_save}.tracking.run_data.richfile'),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fca288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(dir_save).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "roicat.helpers.json_save(obj=results_clusters, filepath=paths_save['results_clusters'])\n",
    "roicat.helpers.json_save(obj=params_used, filepath=paths_save['params_used'])\n",
    "roicat.util.RichFile_ROICaT(path=paths_save['results_all']).save(obj=results_all, overwrite=True)\n",
    "roicat.util.RichFile_ROICaT(path=paths_save['run_data']).save(obj=run_data, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433843e8",
   "metadata": {},
   "source": [
    "Optionally save the FOV_clusters images as a GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9f1c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "roicat.helpers.save_gif(\n",
    "    array=roicat.helpers.add_text_to_images(\n",
    "        images=[(f * 255).astype(np.uint8) for f in FOV_clusters], \n",
    "        text=[[f\"{ii}\",] for ii in range(len(FOV_clusters))], \n",
    "        font_size=3,\n",
    "        line_width=10,\n",
    "        position=(30, 90),\n",
    "    ), \n",
    "    path=str(Path(dir_save).resolve() / 'visualization' / 'FOV_clusters_highQuality.gif'),\n",
    "    frameRate=10.0,\n",
    "    loop=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de25d10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b562fe07",
   "metadata": {},
   "source": [
    "# Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f199a60b",
   "metadata": {},
   "source": [
    "##### Demo: Importing richfiles\n",
    "\n",
    "Simple output files are saved as .json files, which are easily handled. However, complex output files are saved using a custom format called `richfile`, which are basically just structured directories containing data files. This is a demonstration of how to import and handle richfile directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d08b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a richfile object of the results_all dictionary\n",
    "r = roicat.util.RichFile_ROICaT(path=paths_save['results_all'])\n",
    "\n",
    "## You can load it\n",
    "temp_results_all = r.load()\n",
    "\n",
    "## You can load part of it by indexing into the richfile object\n",
    "print(f\"n_sessions: {r['ROIs']['n_sessions'].load()}\")\n",
    "print('')\n",
    "\n",
    "## You can view the tree structure\n",
    "r.view_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e6ae26",
   "metadata": {},
   "source": [
    "#### Look at `params_used`\n",
    "These were the parameters that were used in the run, they can be used to reproduce the run using the CLI by creating a params.yaml file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295abe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac1790a",
   "metadata": {},
   "source": [
    "##### Legacy saving methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52333d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_save = Path('/media/rich/bigSSD/data_tmp/test_data/').resolve()\n",
    "name_save = Path(dir_allOuterFolders).resolve().name\n",
    "\n",
    "path_save = dir_save / (name_save + '.ROICaT.tracking.results' + '.pkl')\n",
    "print(f'path_save: {path_save}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d8df54",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_sd = {\n",
    "    'data': data.serializable_dict,\n",
    "    'aligner': aligner.serializable_dict,\n",
    "    'blurrer': blurrer.serializable_dict,\n",
    "    'roinet': roinet.serializable_dict,\n",
    "    'swt': swt.serializable_dict,\n",
    "    'sim': sim.serializable_dict,\n",
    "    'clusterer': clusterer.serializable_dict,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225b9fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "roicat.helpers.pickle_save(\n",
    "    obj=results_all,\n",
    "    filepath=path_save,\n",
    "    mkdir=True,\n",
    ")\n",
    "\n",
    "roicat.helpers.pickle_save(\n",
    "    obj=run_data_sd,\n",
    "    filepath=str(dir_save / (name_save + '.ROICaT.tracking.rundata' + '.pkl')),\n",
    "    mkdir=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81150d57",
   "metadata": {},
   "source": [
    "Optionally save results as a matlab file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f48da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "roicat.helpers.matlab_save(\n",
    "    obj=results_all,\n",
    "    filepath=str(dir_save / (name_save + '.ROICaT.tracking.results' + '.mat')),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11002b0d",
   "metadata": {},
   "source": [
    "## Save more figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5736f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save gifs of the FOVs at different stages of alignment\n",
    "roicat.helpers.save_gif(\n",
    "    array=roicat.helpers.add_text_to_images(\n",
    "        images=[((f / np.max(f)) * 255).astype(np.uint8) for f in FOV_images], \n",
    "        text=[[f\"{ii}\",] for ii in range(len(FOV_clusters))], \n",
    "        font_size=3,\n",
    "        line_width=10,\n",
    "        position=(30, 90),\n",
    "    ), \n",
    "    path=str(Path(dir_save).resolve() / 'visualization' / 'FOV_images.gif'),\n",
    "    frameRate=10,\n",
    "    loop=0,\n",
    ")\n",
    "\n",
    "roicat.helpers.save_gif(\n",
    "    array=roicat.helpers.add_text_to_images(\n",
    "        images=[((f / np.max(f)) * 255).astype(np.uint8) for f in aligner.ims_registered_geo], \n",
    "        text=[[f\"{ii}\",] for ii in range(len(FOV_clusters))], \n",
    "        font_size=3,\n",
    "        line_width=10,\n",
    "        position=(30, 90),\n",
    "    ), \n",
    "    path=str(Path(dir_save).resolve() / 'visualization' / 'FOV_images_aligned_geometric.gif'),\n",
    "    frameRate=10,\n",
    "    loop=0,\n",
    ")\n",
    "\n",
    "roicat.helpers.save_gif(\n",
    "    array=roicat.helpers.add_text_to_images(\n",
    "        images=[((f / np.max(f)) * 255).astype(np.uint8) for f in aligner.ims_registered_nonrigid], \n",
    "        text=[[f\"{ii}\",] for ii in range(len(FOV_clusters))], \n",
    "        font_size=3,\n",
    "        line_width=10,\n",
    "        position=(30, 90),\n",
    "    ), \n",
    "    path=str(Path(dir_save).resolve() / 'visualization' / 'FOV_images_aligned_nonrigid.gif'),\n",
    "    frameRate=10,\n",
    "    loop=0,\n",
    ")\n",
    "\n",
    "roicat.helpers.save_gif(\n",
    "    array=roicat.helpers.add_text_to_images(\n",
    "        images=[((f / np.max(f)) * 255).astype(np.uint8) for f in aligner.get_ROIsAligned_maxIntensityProjection(normalize=True)], \n",
    "        text=[[f\"{ii}\",] for ii in range(len(FOV_clusters))], \n",
    "        font_size=3,\n",
    "        line_width=10,\n",
    "        position=(30, 90),\n",
    "    ), \n",
    "    path=str(Path(dir_save).resolve() / 'visualization' / 'FOV_ROIs_aligned.gif'),\n",
    "    frameRate=10,\n",
    "    loop=0,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
